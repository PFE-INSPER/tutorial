{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36c1f63a",
   "metadata": {},
   "source": [
    "# Coleta de dados para treinamento de modelo de espectro político para NLU\n",
    "\n",
    "Modelo de classificação de espectro politico, a partir de noticias\n",
    "\n",
    "### Direita\n",
    "\n",
    "- Jovem Pan\n",
    "\n",
    "### Esquerda\n",
    "\n",
    "- Carta Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969da594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "training_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4030c2",
   "metadata": {},
   "source": [
    "## Jovem Pan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9107f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for page in tqdm(range(0, 15)):\n",
    "    html = urlopen(f\"https://jovempan.com.br/noticias/politica/page/{page}\")\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    linhas = bs.find_all('h2', {'class':'post-title'})\n",
    "    for i in linhas:\n",
    "        links.append(str(i.contents[0]).split('\"')[1])\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313c63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in tqdm(links):\n",
    "    html = urlopen(link)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    linhas = bs.find_all('div', {'class':'context'})\n",
    "    text = \"\"\n",
    "    for i in linhas:\n",
    "        for t in i.findChildren(\"p\"):\n",
    "            p = re.compile(r'<.*?>')\n",
    "            res = p.sub('', str(t))\n",
    "            text += res\n",
    "    n = 2000 # chunk length\n",
    "    chunks = [{\"text\": text[i:i+n], \"labels\": [\"Direita\"]} for i in range(0, len(text), n)]\n",
    "    training_data.extend(chunks)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698261d5",
   "metadata": {},
   "source": [
    "## Carta Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed897fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=r'./chromedriver.exe')\n",
    "driver.get('https://www.cartacapital.com.br/mais-recentes/')\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9e7f5",
   "metadata": {},
   "source": [
    "### Pegando os links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9665ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não precisa dessa parte usando o Jupyter\n",
    "\n",
    "def scroll_down():\n",
    "    scroll_pause_time = 2 \n",
    "    i = 3\n",
    "\n",
    "    while True:\n",
    "        screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))\n",
    "        i += 3\n",
    "        time.sleep(scroll_pause_time)\n",
    "\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # Após descer a página ele clica no botão ler mais\n",
    "        try: \n",
    "            btn_load_more = driver.find_element_by_class_name(\"eltdf-bnl-load-more\")\n",
    "            btn_load_more.click()\n",
    "\n",
    "            time.sleep(scroll_pause_time * 3)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if (screen_height) * i > scroll_height:\n",
    "            break\n",
    "\n",
    "j = 0\n",
    "while j < 5:\n",
    "    scroll_down()\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aed18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "mat = bs.find(\"div\", class_=\"eltdf-bnl-outer\")\n",
    "links = [h3.find(\"a\").get(\"href\") for h3 in mat.find_all(\"h3\")]\n",
    "\n",
    "with open('links.txt', 'w') as file: \n",
    "    for link in links:\n",
    "        content = link + '\\n' \n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc0e88f",
   "metadata": {},
   "source": [
    "### Pegando Conteudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c554f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1306fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('links.txt', 'r') as file:\n",
    "    links = [line[:-1] for line in file.readlines()]\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = ['Um minuto, por favor...', 'Obrigado por ter chegado até aqui. Combater a desinformação, as mentiras e os ataques às instituições custa tempo e dinheiro. Nós, da CartaCapital, temos o compromisso diário de levar até os leitores um jornalismo crítico, alicerçado em dados e fontes confiáveis. Acreditamos que este seja o melhor antídoto contra as fake news e o extremismo que ameaçam a liberdade e a democracia.', \n",
    "'Se você acredita no nosso trabalho, junte-se a nós. Apoie, da maneira que puder. Ou assine e tenha acesso ao conteúdo integral de CartaCapital!', 'ou, se preferir,']\n",
    "\n",
    "for url in links:\n",
    "    r = requests.get(url).content\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "\n",
    "    article = soup.find(\"article\")\n",
    "    ps = article.find_all(\"p\")\n",
    "\n",
    "    strings = [p.string for p in ps if p.string != None and p.string not in lista]\n",
    "    text = \"\".join(strings)\n",
    "\n",
    "    n = 2000 # chunk length\n",
    "    chunks = [{\"text\": text[i:i+n], \"labels\": [\"Esquerda\"]} for i in range(0, len(text), n)]\n",
    "    training_data.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce84d2",
   "metadata": {},
   "source": [
    "## Salva Arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1652ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_filename = 'training_data.json'\n",
    "with open(training_data_filename, 'w+', encoding='utf-8') as f:\n",
    "    json.dump(training_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print('Data successfully saved locally in ' + training_data_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
